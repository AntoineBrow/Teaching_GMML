{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernel Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this class is to implement the _Kernel Trick_ in two applications:\n",
    "* Kernel Ridge Regression\n",
    "* Kernel PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We consider some data $x_1, \\ldots, x_n \\in \\mathbb{R}$, with corresponding observations $y_1, \\ldots, y_n \\in \\mathbb{R}$. Our goal is to regress the observations $y$ on the data $X = [x_1, \\ldots, x_n]^T \\in \\mathbb{R}^{n \\times 1}$.\n",
    "\n",
    "In this experiment, we consider noisy and non-linear observations $y_i$ from the model:\n",
    "$$ y_i = \\sin(x_i) + \\sigma \\xi_i $$\n",
    "where $\\sigma > 0$ is the noise level and $\\xi_i \\sim \\mathcal{N}(0,1)$ are i.i.d."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first use a classical Ridge Regression, and then a Kernel Ridge Regression. In the classical Ridge Regression, the goal is to find a parameter $\\theta \\in \\mathbb{R}$ that minimizes the loss\n",
    "$$ \\|y - X\\theta\\|^2 + \\rho \\|\\theta\\|^2 $$\n",
    "\n",
    "The solution is\n",
    "$$ \\hat\\theta = TODO $$\n",
    "\n",
    "This only depends on the scalar products between the data points. If we replace the dot product by a p.d. kernel, we obtain the Kernel Ridge Regressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the data\n",
    "n = 100\n",
    "rho = 1 # Ridge regression parameter\n",
    "\n",
    "X = np.random.uniform(low=-np.pi, high=np.pi, size=n)\n",
    "\n",
    "sigma = 0.2\n",
    "xi = np.random.randn(n)\n",
    "\n",
    "y = np.sin(X) + sigma*xi\n",
    "\n",
    "# Plot the data\n",
    "plt.figure(figsize=(17,6))\n",
    "plt.scatter(X, y, s=50, label='Data')\n",
    "lin = np.linspace(-np.pi, np.pi, 100)\n",
    "plt.plot(lin, np.sin(lin), c='r', lw=7, label='Ground truth')\n",
    "plt.legend(loc='best', fontsize=25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learn a Ridge regressor\n",
    "w = # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the predictions\n",
    "plt.figure(figsize=(17,6))\n",
    "plt.scatter(X, y, s=50, label='Data')\n",
    "lin = np.linspace(-np.pi, np.pi, 100)\n",
    "plt.plot(lin, np.sin(lin), c='r', lw=7, label='Ground truth')\n",
    "plt.plot(lin, # TODO, c='g', lw=7, label='Ridge regressor')\n",
    "plt.legend(loc='best', fontsize=25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the Gaussian Kernel:\n",
    "$$ k_\\tau(x,y) = \\exp\\left( -\\frac{\\|x-y\\|^2}{\\tau^2} \\right) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel(x, y, tau=1):\n",
    "    return np.exp(-np.linalg.norm(x-y)**2/(tau**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the Ridge Regressor\n",
    "K = # TODO\n",
    "w = # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the preditions\n",
    "plt.figure(figsize=(17,6))\n",
    "plt.scatter(X, y, s=50, label='Data')\n",
    "lin = np.linspace(-np.pi, np.pi, 100)\n",
    "plt.plot(lin, np.sin(lin), c='r', lw=7, label='Ground truth')\n",
    "plt.plot(lin, # TODO, c='g', lw=7, label='Ridge regressor')\n",
    "plt.legend(loc='best', fontsize=25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We consider $n$ data points dimension $p$: $X \\in \\mathbb{R}^{n \\times p}$. We wish to reduce the dimension of our data, from $p$ to $s << p$.\n",
    "\n",
    "The goal of the PCA is to find the $s$-dimensional subspace $E$ that \"best\" fit the data, i.e. that minimizes the loss:\n",
    "$$ \\frac{1}{n} \\sum_{i=1}^n \\|x_i - \\Pi_E(x_i)\\|^2 $$\n",
    "where $\\Pi_E$ is the orthogonal projector onto $E$.\n",
    "\n",
    "Using the Pythagorean theorem, this is equivalent to find a $s$-dimensional subspace $E$ that maximizes the quantity:\n",
    "$$ \\frac{1}{n} \\sum_{i=1}^n \\|\\Pi_E(x_i)\\|^2 = \\text{Trace}\\left(\\Pi_E \\left[\\frac{1}{n} \\sum_{i=1}^n x_i x_i^T\\right] \\Pi_E \\right)$$\n",
    "\n",
    "This quantity is maximized when $E$ is spanned by the top $s$ eigenvectors (i.e. the $s$ eigenvectors associated with the largest $s$ eigenvalues) of the variance matrix $\\Sigma = \\frac{1}{n} \\sum_{i=1}^n x_i x_i^T$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We consider 2-dimensional data with two classes (red and blue)\n",
    "# Out goal will be to project this data on a 1-dimensional subspace\n",
    "# and still be able to cluster the data correctly\n",
    "from sklearn.datasets import make_circles\n",
    "\n",
    "n = 100\n",
    "\n",
    "X, y = make_circles(n_samples=n, factor=.3, noise=.05)\n",
    "\n",
    "plt.figure(figsize=(7,7))\n",
    "plt.scatter(X[y==0, 0], X[y==0, 1], c=\"red\", s=250)\n",
    "plt.scatter(X[y==1, 0], X[y==1, 1], c=\"blue\", s=250)\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sigma = # TODO\n",
    "projected_points = # TODO\n",
    "\n",
    "plt.figure(figsize=(7,7))\n",
    "plt.scatter(projected_points[y==0, 0], projected_points[y==0, 1], c=\"red\", s=250, alpha=0.5)\n",
    "plt.scatter(projected_points[y==1, 0], projected_points[y==1, 1], c=\"blue\", s=250, alpha=0.5)\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, PCA does not allow us to separate the data, because the dimensionality reduction is linear. We will use the Kernel Trick to alleviate this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each datum $x_i$ is now sent to a Hilbert Space $\\mathcal{H}$ through a non-linear map $\\phi: \\mathbb{R}^p \\to \\mathcal{H}$. We then compute the linear PCA in this space $\\mathcal{H}$.\n",
    "\n",
    "We have to eigendecompose the operator $\\Sigma = \\frac{1}{n} \\sum_{i=1}^n \\phi(x_i) \\phi(x_i)^T$, where $\\phi(x_i) \\phi(x_i)^T$ should be thought as the operator $x \\to \\phi(x_i) \\langle \\phi(x_i), x \\rangle_\\mathcal{H}$.\n",
    "\n",
    "Note that eigenvectors of $\\Sigma$ are necessarily elements of $\\text{Span}\\left( \\phi(x_1), \\ldots, \\phi(x_n) \\right)$. This means that, for an eigenvector $v$ with corresponding eigenvalue $\\lambda$:\n",
    "* there exist coefficients $\\alpha_1, \\ldots \\alpha_n$ such that $v = \\sum_{i=1}^n \\alpha_i \\phi(x_i)$,\n",
    "* the eigenvector $v$ is characterized by the equations $\\langle \\phi(x_k) , \\Sigma v \\rangle_\\mathcal{H} = \\lambda \\langle \\phi(x_k) , v \\rangle_\\mathcal{H}$ for all $k=1,\\ldots,n$.\n",
    "\n",
    "Combining those two results gives:\n",
    "$$ K^2 \\alpha = n \\lambda K \\alpha $$\n",
    "where $K \\in \\mathbb{R}^{n \\times n}$ is the Kernel matrix, and then $K \\alpha = n \\lambda \\alpha$. In other words, the coefficients vector $\\alpha$ is an eigenvector of the Kernel matrix $K$.\n",
    "\n",
    "In PCA, we ask that the eigenvectors have norm equal to $1$. Here, it means that $\\|v\\| = 1$, which translates into $\\|\\alpha\\| = \\frac{1}{\\sqrt{\\lambda}}$.\n",
    "\n",
    "Finally, we also ask that the initial data are centered. We have to ensure that $\\sum_{i=1}^n \\phi(x_i) = 0_\\mathcal{H}$. Centering the data in $\\mathcal{H}$ can be shown to be equivalent to using a Kernel matrix $\\tilde K = K - 1 K - K 1 + 1 K 1$, where $1 \\in \\mathbb{R}^{n \\times n}$ is the matrix with coefficients $1/n$.\n",
    "\n",
    "In order to compute the projection of a point $x \\in \\mathbb{R}^p$ onto the first principal component, we need to compute:\n",
    "$$ \\langle v , \\phi(x) \\rangle_\\mathcal{H} = \\sum_{i=1}^n \\alpha_i k(x_i, x)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel(x, y, tau=0.5):\n",
    "    return np.exp(-np.linalg.norm(x-y)**2/(tau**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = # TODO\n",
    "projected_points = # TODO\n",
    "\n",
    "plt.figure(figsize=(7,7))\n",
    "plt.scatter(projected_points[0][y==0], projected_points[1][y==0], c='red', s=250, alpha=0.3)\n",
    "plt.scatter(projected_points[0][y==1], projected_points[1][y==1], c='blue', s=250, alpha=0.3)\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kernel PCA separates the data using only one dimension."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
